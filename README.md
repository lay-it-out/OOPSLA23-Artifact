# Artifact of paper "Automated Ambiguity Detection in Layout-Sensitive Grammars"

This is the artifact for the paper "Automated Ambiguity Detection in Layout-Sensitive Grammars" at OOPSLA'23.
It consists of two parts:
1. `lamb/`: our prototype tool that implements the ambiguity detection method introduced in sections ???, together with necessary data and script for reproducing the evaluation in section ???;
2. `coq/`: our Coq formulation of the local ambiguity (), SMT encoding (), and proofs of all the lemmas/theorems.

## 1 Getting Started Guide (??? min)

### 1.1 Building Lamb (??? min)

We provide two options to set up the environment and you should choose **either**:

- (1.1a) via [Nix]() (version ??? is tested; latest recommended)
- (1.1b) via [Virtual Machine](https://www.virtualbox.org)

#### 1.1a

If you're using Unix/Linux, option 1.1a is much easier as installing the Nix package manager is usaually a easy job, via the following command in your shell:

```bash
curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install
```

Afterwards, set your current directory to the artifact directory (`lamb-artifact`) and run the following to build the artifact:

```bash
nix build
```

The building process should take no more than 15 minutes. If an error occurs asking you to enable experimental features of nix, refer to [this guide](https://nixos.wiki/wiki/Flakes#Enable_flakes) to enable flakes and nix commands.

#### 1.1b

### 1.2 Testing the Motivating Example

Once you have successfully built Lamb, try running the motivating example. File `tests/motivating-example.bnf` contains the grammar used in Section 2 of the paper. First, run our tool on that grammar (assuming that your current directory is `lamb-artifact`):

```
nix run . -- tests/motivating-example.bnf
```

The output should look like below

```
...lines of solving process omitted...

***
Ambiguous sentence of length 3 found. It shall be listed below.
***

do
nop
nop

***
Found locally ambiguous variable: "new-var-0". It corresponds to token(s) [1, 3] in the ambiguous sentence.
***

NOTE: indexing for tokens in the sentence starts at 1. Spaces in the sentence are denoted as `␣'.
NEXT STEP: List and review all parse trees. Type help for available commands. Command completion available with TAB key.

Now entering REPL...
smt-ambig>
```

This indicates that our tool successfully found an ambiguous sentence:

```
do
nop
nop
```

As mentioned in the paper, we only consider dissimilar subtrees, since this (in combination with the reachability condition) is logically equivalent to derivation ambiguity. Considering the dissimilar subtrees that witnesses local ambiguity, two of them exists for the subword $w^{1\dots3}$, and they both has the variable $\text{new-var-0}$ as their root. $\text{new-var-0}$ is a new variable created in the process of translating EBNF grammar into LS2NF grammar. When presenting the parse trees, this transformation is undone -- you'll only see variables and rules in original EBNF grammar in the presented parse trees.

First, type `list tree new-var-0` and press enter to ensure that there exists two parse trees for the given sentence:

```
smt-ambig> list tree new-var-0
Parse trees of nonterminal: new-var-0
+-------+---------------------+
| index |         type        |
+-------+---------------------+
|   0   | UnaryExpressionNode |
|   1   | UnaryExpressionNode |
+-------+---------------------+
```

Please pay attention to how the common tree root $\text{new-var-0}$ is used in the command to denote the parse tree forest.

Then, have a look at the parse trees one by one, by using the commands `show tree new-var-0 0` and `show tree new-var-0 1`.  Press enter after each command. You should see the parse trees generated by our tool Lamb.

Finally, input `exit` to quit our tool.

### 1.3 Checking the Coq Proof Artifact

## 2 Step-by-Step Instructions for Reproducing Evaluations (~??? min)

### 2.1 Looking at Dataset

TODO:
- Introduce the location of the dataset shown in Table 1. 
- Move the introduction of EBNF here.
- Say that in each lang group, the first test case is the original, and the other four are mutants.

### 2.2 Running the Experiments

We assume that you are currently at the `lamb-artifact` folder. First, load the development shell for our tool:

```bash
nix develop
```

Then, execute the following command to run all but Python / SASS testcases.

```bash
python run_tests.py
```

This shall take around 10-25 minutes. Afterwards, the total running time (sum of `solve_time` and `other_time`), ambiguous sentence length, among other details, shall be printed to the terminal. Another copy will also be stored into `result.json` in the current folder.

To run every testcases including Python and SASS, please run the following command.

```bash
python run_tests.py --all
```

Note that this can take very long (>24h) to complete.

TODO:
1. Set up a timeout in our experiment script, so that we hopefully to reproduce everything within a few hours, not a day.
2. If the reviewer does not want to run all, provide a script to execute a set of quick examples -- maybe the two fastest examples from each lang? -- so that everything is done in one hour.

### 2.3 Checking the Results

The results of step 2.2 are saved into a CSV file ???. It has the exact same structure with Table 1 in our paper.

Note: Despite the randomness of the found satisfiable model by Z3 solver, the lengths of the ambiguous sentence (i.e., the last column of Table 1) are determined, though under the hood the found ambiguous sentence may differ.

TODO: we should have a script that does the above, which saves reviewer's time to check data.

### 2.4 Human Understanding of Ambiguous Sentence

This section presents how one can analyze the cause of ambiguity by inspecting the parse trees for the generated ambiguous sentence, as mentioned in section 7.2. This is a minor result of our evaluation: skip it if your time is tight.

TODO: details

### 2.5 Relating Coq Definitions/Theorems to Paper